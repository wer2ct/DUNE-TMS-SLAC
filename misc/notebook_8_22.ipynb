{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3cd47623-1b8b-4766-8216-7b464fddd8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import ROOT as root\n",
    "import awkward as ak\n",
    "import uproot\n",
    "from array import array\n",
    "import torch\n",
    "import networkx as nx\n",
    "from scipy.spatial import Delaunay\n",
    "from torch_geometric.data import Data, Dataset\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e528dd86-6345-4bee-8eaf-3cd0bcb21f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddInfo(dbscan_hits, const_hit_array, edep_events):\n",
    "    event = root.TG4Event()\n",
    "    edep_events.SetBranchAddress(\"Event\",root.AddressOf(event))\n",
    "    dbscan_cluster_list = []\n",
    "    spills_in_file = np.unique(dbscan_hits[:,-3])\n",
    "    iter_ = 0\n",
    "    for spill_number in range(2): #spills_in_file:\n",
    "        spill_level = dbscan_hits[dbscan_hits[:,-3] == spill_number]\n",
    "        segments_in_spill = np.unique(spill_level[:,-2])\n",
    "        for segment_number in segments_in_spill:\n",
    "            time_segment_level = spill_level[spill_level[:,-2] == segment_number]\n",
    "            clusters_in_segment = np.unique(time_segment_level[:,-1])\n",
    "            for cluster_number in clusters_in_segment:\n",
    "                dbscan_cluster_level = time_segment_level[time_segment_level[:,-1] == cluster_number]\n",
    "                rows = []\n",
    "                #alright, for each hit can pull the m-nn, m-hn. \n",
    "                expected_hits = len(dbscan_cluster_level)\n",
    "                for i, hit in enumerate(dbscan_cluster_level):\n",
    "                    #assign refs \n",
    "                    m_nn = hit[0]\n",
    "                    m_hn = hit[1]\n",
    "                    ref_nn = m_nn\n",
    "                    ref_hn = m_hn\n",
    "                \n",
    "                    #grab the const hit array\n",
    "                    const_hit_array_nn = const_hit_array[const_hit_array[:,0] == m_nn]\n",
    "                    const_hit_info = const_hit_array_nn[const_hit_array_nn[:,1] == m_hn]\n",
    "                \n",
    "                    #pull the default information. \n",
    "                    edep_events.GetEntry(int(m_nn)) #assign the event. Gets called at least once per hit.  \n",
    "                    #print(f'{m_nn} and {m_hn}')\n",
    "                    default_trackid = int(hit[2])\n",
    "                    default_pdgid = ((event.Trajectories)[default_trackid]).GetPDGCode() #pull only one trajectory and pdgid we are interested in. \n",
    "\n",
    "                    #ok now we have the defaults. Two paths:\n",
    "\n",
    "                    #If our pdgid is != 13, AND we have constituent hits to scan over, then lets scan over them.\n",
    "                    if (abs(default_pdgid) != 13) and (np.shape(const_hit_info)[0] > 1):\n",
    "                        #loop over the constituent hit nns, want to limit get entry calls\n",
    "                        Replaced = False #monitors state, helps us break early or make sure to add our row at the end of no replacement is found. \n",
    "                        for constituent_hit_nn in np.unique(const_hit_info[:,2]):\n",
    "                            #we have to loop over neutrino number since we have instances of merged hits w/ multiple neutrino events. \n",
    "                            #check state, and if it has already been replaced, continue no further. \n",
    "                            if Replaced == True:\n",
    "                                break\n",
    "                            edep_events.GetEntry(int(constituent_hit_nn)) #set the event. \n",
    "                            const_nn_sub_array = const_hit_info[const_hit_info[:,2] == constituent_hit_nn ] #grab the constituent hits w/ the desired neutrino number\n",
    "                            for const_hit in const_nn_sub_array:\n",
    "                                #another check of state!\n",
    "                                if Replaced == True:\n",
    "                                    break\n",
    "                                \n",
    "                                const_hit_number = const_hit[3]\n",
    "                                const_trackid = int(const_hit[4])\n",
    "                                const_pdgid = ((event.Trajectories)[const_trackid]).GetPDGCode() #pull only one trajectory and pdgid we are interested in.\n",
    "                            \n",
    "                                #ok, so we've found an instance of a constituent w/ a pdgid of 13! Let's save our row. \n",
    "                                if (abs(const_pdgid) == 13):\n",
    "                                    Replaced = True #set the state. \n",
    "                                    new_row = np.array((constituent_hit_nn, const_hit_number, hit[4], hit[3], hit[6], const_trackid, const_pdgid))\n",
    "                                    rows.append(new_row)\n",
    "                                    iter_ +=1 \n",
    "\n",
    "                        #say we have loop over all constituents and found no muons, despite there being constituents, means our Replaced will still be False\n",
    "                        if (Replaced == False):\n",
    "                            #if found no replacement despite not being a muon by default, and having constituents, just append the defaults. \n",
    "                            new_row = np.array((ref_nn, ref_hn, hit[4], hit[3], hit[6], default_trackid, default_pdgid))\n",
    "                            rows.append(new_row)\n",
    "                            iter_ +=1 \n",
    "    \n",
    "                    #Else -> Ie, our pdgid == 13, OR our pdgid != 13 but there is only 1 constituent hit, nothing to scan over, just save what we have as default. \n",
    "                    else:\n",
    "                        new_row = np.array((ref_nn, ref_hn, hit[4], hit[3], hit[6], default_trackid, default_pdgid))\n",
    "                        rows.append(new_row)\n",
    "                        iter_ +=1\n",
    "\n",
    "                    if (iter_ % 1000 == 0):\n",
    "                        print(f'Added info through - {iter_}')\n",
    "\n",
    "                #add to our running list, now grouped by cluster. \n",
    "                infod_hits = len(rows)\n",
    "                if infod_hits != expected_hits:\n",
    "                    print(f\"Iter - {iter}, Expected {expected_hits}, got {infod_hits} - something is wrong!\")\n",
    "                dbscan_hits_with_info = np.vstack(rows)\n",
    "                dbscan_cluster_list.append(dbscan_hits_with_info)\n",
    "                    \n",
    "    return(dbscan_cluster_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3cdfd42-5407-4b6a-86b7-42aa1b6ac035",
   "metadata": {},
   "outputs": [],
   "source": [
    "detsim_file = uproot.open(\"/sdf/data/neutrino/summer25/ktwall/full_spill_detsim/detsim_2500.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8206be4c-79bf-4488-980d-503cfe89aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tree = detsim_file[\"MergedTree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3460974a-efe2-4a4e-9a6e-2c882e375f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "const_nns = merged_tree[\"constituent_neutrino_numbers\"].array()\n",
    "const_hns = merged_tree[\"constituent_hit_numbers\"].array()\n",
    "const_trackids = merged_tree[\"constituent_hit_trackids\"].array()\n",
    "#want an array like core nn, core hn, const nn, const hn, const trackid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e61bbb0a-f8ea-4551-a5ce-d4242aca9610",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m merged_group_nns \u001b[38;5;241m=\u001b[39m const_nns[i]\n\u001b[1;32m      5\u001b[0m merged_group_hns \u001b[38;5;241m=\u001b[39m const_hns[i]\n\u001b[0;32m----> 6\u001b[0m merged_group_trackids \u001b[38;5;241m=\u001b[39m \u001b[43mconst_trackids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m core_nn \u001b[38;5;241m=\u001b[39m merged_group_nns[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m#core is definitionally first element of the vector \u001b[39;00m\n\u001b[1;32m      8\u001b[0m core_hn \u001b[38;5;241m=\u001b[39m merged_group_hns[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/awkward/highlevel.py:1124\u001b[0m, in \u001b[0;36mArray.__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ak\u001b[38;5;241m.\u001b[39moperations\u001b[38;5;241m.\u001b[39mak_with_named_axis\u001b[38;5;241m.\u001b[39m_impl(\n\u001b[1;32m   1117\u001b[0m         indexed_layout,\n\u001b[1;32m   1118\u001b[0m         named_axis\u001b[38;5;241m=\u001b[39mNamedAxis\u001b[38;5;241m.\u001b[39mmapping,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs,\n\u001b[1;32m   1122\u001b[0m     )\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_layout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexed_layout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_behavior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_other\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/awkward/_layout.py:224\u001b[0m, in \u001b[0;36mwrap_layout\u001b[0;34m(content, behavior, highlevel, like, allow_other, attrs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mawkward\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhighlevel\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mawkward\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Content\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mawkward\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Record\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, (Content, Record)) \u001b[38;5;129;01mor\u001b[39;00m allow_other\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m behavior \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(behavior, Mapping)\n",
      "File \u001b[0;32m/app/root/lib/ROOT/_facade.py:153\u001b[0m, in \u001b[0;36mROOTFacade._set_import_hook.<locals>._importhook\u001b[0;34m(name, *args, **kwds)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_ihook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "constituency_array_list = []\n",
    "for i in range(len(const_nns)):\n",
    "    merged_group_index = i\n",
    "    merged_group_nns = const_nns[i]\n",
    "    merged_group_hns = const_hns[i]\n",
    "    merged_group_trackids = const_trackids[i]\n",
    "    core_nn = merged_group_nns[0] #core is definitionally first element of the vector \n",
    "    core_hn = merged_group_hns[0]\n",
    "    for j in range(1,len(merged_group_nns)): #loop over remaining indices, these are constituents. Only runs if more than one entry. \n",
    "        constituency_info = [core_nn,core_hn, merged_group_nns[j], merged_group_hns[j], merged_group_trackids[j]]\n",
    "        constituency_array_list.append(constituency_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bc6473a-6ded-47be-8d4a-ca948a6f8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "constituency_array = np.vstack(constituency_array_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74abec89-509d-4da1-afc4-f5c092bf6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_sub = constituency_array[constituency_array[:,0] == 654]\n",
    "hit_sub = nn_sub[nn_sub[:,1] == 1102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9fa94958-30db-4c7e-a74f-c0c0226ee02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_file = np.load(\"/sdf/home/k/ktwall/hits_clustered_epsilon_0.1_2500.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2aac89d6-8e40-42bb-bb9b-1b70b907d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "const_array = clustered_file[\"constituents_array\"]\n",
    "dbscan_hits = clustered_file[\"dbscan_hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1be2382c-f207-4a82-bb33-0c98f650eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = root.TFile.Open(\"/sdf/data/neutrino/summer25/tanaka/nd-production/run-spill-build/MicroProdN4p1_NDComplex_FHC.spill.full/EDEPSIM_SPILLS/0002000/0002500/MicroProdN4p1_NDComplex_FHC.spill.full.0002500.EDEPSIM_SPILLS.root\")\n",
    "edep_events = f.Get(\"EDepSimEvents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "57850095-5e99-4971-854e-cc8f8a5c9990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our labeling logic. \n",
    "def AssignLabels(dbscan_cluster_list, edep_events):\n",
    "    #initialize \n",
    "    event = root.TG4Event()\n",
    "    edep_events.SetBranchAddress(\"Event\",root.AddressOf(event))\n",
    "    #separate our cluster by trackid - first step regardless. \n",
    "    labeled_slice_list = []\n",
    "    for chosen_slice in range(len(dbscan_cluster_list)):\n",
    "        slice_hits = dbscan_cluster_list[chosen_slice]\n",
    "        trackid_separated = []\n",
    "        #Yep, should still scan for neutrino number\n",
    "        for nn in np.unique(slice_hits[:,0]):\n",
    "            nn_sub_array = slice_hits[slice_hits[:,0] == nn] #pull the hits in our slice w/ a given neutrino #. Should never group trackids across neutrino numbers together. \n",
    "            for trackid in np.unique(nn_sub_array[:,-2]): #switch the indexing \n",
    "                trackid_sub_array = nn_sub_array[nn_sub_array[:,-2] == trackid]\n",
    "                trackid_separated.append(trackid_sub_array)\n",
    "                \n",
    "        #this statement need to be explicit or it can lead to errors for some reason, break into basic tracks and basic showers\n",
    "        basic_tracks = []\n",
    "        basic_showers = []\n",
    "        for trackid_group in trackid_separated:\n",
    "            if np.shape(trackid_group)[0] > 4:\n",
    "                basic_tracks.append(trackid_group)\n",
    "            if np.shape(trackid_group)[0] <= 4:\n",
    "                basic_showers.append(trackid_group)\n",
    "\n",
    "        #create primary track list!\n",
    "        primary_track_list = []\n",
    "        for track_group in basic_tracks:\n",
    "            primary_track_list.append( (int(track_group[0][0]), int(track_group[0][-2])) ) #nn and trackid? \n",
    "\n",
    "        advanced_tracks = basic_tracks  \n",
    "        advanced_showers = []\n",
    "\n",
    "        #local context, \n",
    "        for group in basic_showers:\n",
    "            nn = group[0][0] #grab neutrino # for group\n",
    "            trackid = group[0][-2] #grab the trackid for the group\n",
    "            #now quickly grab the trackids of our primary track groups associated w/this event, for use\n",
    "            edep_events.GetEntry(int(nn))\n",
    "            event_trajectories = event.Trajectories #fetch trajectories vector\n",
    "            group_traj = event_trajectories[int(trackid)] #our root trajectory \n",
    "            group_traj_parent = group_traj.GetParentId()\n",
    "            parent_tuple_reference = (int(nn), int(group_traj_parent))\n",
    "            added = False\n",
    "    \n",
    "            for i, ptref in enumerate(primary_track_list): #i will key which index of the track list to add to. \n",
    "                #check first if the parent matches\n",
    "                if ptref == parent_tuple_reference:\n",
    "                    #print('found a shower w/ a main track parent')\n",
    "                    #print(f'{parent_tuple_reference} and {ptref}')\n",
    "                    advanced_tracks[i] = (np.vstack((basic_tracks[i], group)) )\n",
    "                    added = True\n",
    "          \n",
    "                #if not, check if the grandparent matches, to expand tracks even further. \n",
    "                if added ==  False:\n",
    "                    group_traj_grandparent = event_trajectories[int(group_traj_parent)].GetParentId()\n",
    "                    grandparent_tuple_reference = (int(nn), int(group_traj_grandparent))\n",
    "                    if ptref == grandparent_tuple_reference:\n",
    "                        advanced_tracks[i] = (np.vstack((basic_tracks[i], group)) )\n",
    "                        added = True\n",
    "       \n",
    "                #if not check if the great grandparent matches, expands it even further!! This one may be a bit of a step far. \n",
    "        \n",
    "                if added ==  False:\n",
    "                    group_traj_great_grandparnet = event_trajectories[int(group_traj_grandparent)].GetParentId()\n",
    "                    great_grandparent_tuple_reference = (int(nn), int(group_traj_great_grandparnet))\n",
    "                    if ptref == great_grandparent_tuple_reference:\n",
    "                        advanced_tracks[i] = (np.vstack((basic_tracks[i], group)) )\n",
    "                        added = True\n",
    "    \n",
    "            if added == False:\n",
    "                advanced_showers.append(group)\n",
    "\n",
    "        #lets assign labels and concatenate, track = 0, else = 1, also collapse down to (z,x,PE,label)\n",
    "        to_stack = []\n",
    "        if len(advanced_tracks) > 0:\n",
    "            advanced_tracks_array = np.vstack(advanced_tracks)\n",
    "            tracks_labeled = np.column_stack(((advanced_tracks_array[:,2], advanced_tracks_array[:,3], advanced_tracks_array[:,4], np.zeros_like(advanced_tracks_array[:,0]))))\n",
    "            to_stack.append(tracks_labeled)\n",
    "        if len(advanced_showers) > 0:\n",
    "            advanced_showers_array = np.vstack(advanced_showers)\n",
    "            showers_labeled = np.column_stack(((advanced_showers_array[:,2],advanced_showers_array[:,3],advanced_showers_array[:,4], np.ones_like(advanced_showers_array[:,0]))))\n",
    "            to_stack.append(showers_labeled)\n",
    "        \n",
    "        #stack into an array. \n",
    "        slice_labeled = np.vstack(to_stack)\n",
    "        #now add to the list\n",
    "        labeled_slice_list.append(slice_labeled)\n",
    "\n",
    "        if (chosen_slice % 100 == 0):\n",
    "            print(f'Assigned labels through {chosen_slice}')\n",
    "        \n",
    "    return(labeled_slice_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6a7b763d-3da5-45a8-95d4-1742a948ab21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added info through - 1000\n",
      "Added info through - 2000\n",
      "Added info through - 3000\n",
      "Added info through - 4000\n",
      "Added info through - 5000\n",
      "Added info through - 6000\n",
      "Added info through - 7000\n",
      "Added info through - 8000\n",
      "Added info through - 9000\n",
      "Added info through - 10000\n",
      "Added info through - 11000\n",
      "Added info through - 12000\n",
      "Added info through - 13000\n",
      "Added info through - 14000\n",
      "Added info through - 15000\n"
     ]
    }
   ],
   "source": [
    "dbscan_cluster_info_list = AddInfo(dbscan_hits, const_array, edep_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ea2dc21-3762-425d-bdfa-8376ee5e88af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned labels through 0\n",
      "Assigned labels through 100\n",
      "Assigned labels through 200\n",
      "Assigned labels through 300\n",
      "Assigned labels through 400\n",
      "Assigned labels through 500\n"
     ]
    }
   ],
   "source": [
    "labeled_hits = AssignLabels(dbscan_cluster_info_list,edep_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25bd5dc-a139-4397-9edd-2cc7631268c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
